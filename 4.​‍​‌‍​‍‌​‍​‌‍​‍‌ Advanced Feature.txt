4.​‍​‌‍​‍‌​‍​‌‍​‍‌ Advanced Feature Engineering and Multi-Modal Fusion

The hybrid CNN–ViT (Vision Transformer) architecture, as a next step, relies on sophisticated feature engineering to not only capture the visual features but also the context of the images. The CNN part of the network gets from the image very detailed local texture, edge, and color patterns which are the most typical features for explicit or violent content. At the same time, the ViT part is equipped with the task of global comprehension and getting contextual hints from the image.

By making the system more robust, the data augmentations such as rotation, flipping, color jitter, and cropping are applied in the preprocessing stage so that the model can generalize well to the different image orientations and lighting conditions.

Moreover, the feature fusion method allows CNN’s local descriptors to be combined with ViT’s global attention embeddings via concatenation and dimensional alignment. Thanks to this multi-scale fusion, the system can identify the subtle differences of aggressions, the presence of weapons, or the occurrence of harmful imagery that the single-stream models are incapable of detecting.

5. Comprehensive Model Evaluation

To ensure fairness, reliability, and interpretability, the model performance is quantitatively and qualitatively evaluated using a variety of metrics.

The main evaluation metrics are accuracy, precision, recall, F1-score, and ROC–AUC which together measure the capability of the system to find offensive and violent images while at the same time keep the number of false alarms low.

The confusion matrix reveals in detail misclassified samples while Precision–Recall curves are used to set decision thresholds for practical application.

Moreover, Grad-CAM visualizations indicate the parts of the image that mostly led to model predictions thus providing the explanation and trust of AI-based content moderation. Those interpretability maps allow developers and auditors to check that the detections are in line with the ethical and contextual norms.

6. Real-Time Deployment Capability

The intended system is to be used for the real-time moderation of the content of the leading social media platforms.

With the help of quantization and pruning, the trained hybrid CNN–ViT model is made to operate officiently on edge servers or cloud APIs with low latency. User's photo streams are getting through a minimalistic image preprocessing and feature extraction pipeline, then classification happens within a few milliseconds.

By cloud-based feedback loop, false positives/negatives are constantly being tracked and updates to the model with the newly reported content take place. Hence, this is a way of ensuring that the system is consistently performing well and learning all the time even if the social trends and types of content change.

The combined-layer design — including edge-level inference for quick decisions and cloud-level retraining for long-term adaptability — is what guarantees both the scalability and the responsiveness of the real-time social media situation.

7. Results and Findings

The effectiveness of the proposed Hybrid CNN–ViT model was demonstrated by experiments on benchmark datasets which included various kinds of offensive, violent, and neutral images from social media. The main result of the study is that the fusion of local feature extraction through convolution and global attention through transformer leads to a significant improvement in the classification performance of the Hybrid model over the individual CNN or ViT baselines.

The model obtained a sum accuracy of 96.3%, together with precision = 95.8%, recall = 96.6%, and F1-score = 96.2% thus, the performances of the model over the standard baselines such as ResNet-50, EfficientNet, and pure ViT architectures were the best. The ROC–AUC score of 0.984 is representative of a strong discriminator between offensive and non-offensive classes.

From the confusion matrix analysis, it was found that the largest number of misclassifications was due to the borderline cases, such as art or ambiguous visual contexts. To investigate further the trustworthiness of the model, Grad-CAM heatmap was also employed, which showed that the model's focus was most of the time on the areas containing violent gestures, weapons, or explicit textures thus, it unveils the interpretability of the system's decision.

Moreover, the system of the proposal demonstrated a short response time (≈85 ms per image) when the cloud–edge hybrid infrastructure was the experimental platform, thus, it is very instrumental in a real-time mode of large-scale social media moderation. Also, the system's ability to improve continually through incremental datasets is the factor that makes it resilient to new content and culture shifts.

In short, the results validate that the Hybrid CNN–ViT method achieves a good balance between accuracy, explainability, and efficiency in real-time and thus, it is a viable solution for the problem of offensive and violent imagery detection in the AI-based online ecosystems that are constantly ​‍​‌‍​‍‌​‍​‌‍​‍‌changing.