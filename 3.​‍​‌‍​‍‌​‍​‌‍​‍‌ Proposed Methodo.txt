3.​‍​‌‍​‍‌​‍​‌‍​‍‌ Proposed Methodology

3.1 Overview

The intent of the SafeVisionNet system includes the introduction of a hybrid deep learning framework aimed at the identification and subsequent reduction of offensive, violent, and sexually explicit images that are being shared on social media platforms. To this end, the model employs Convolutional Neural Networks (CNNs) for the extraction of localised spatial features while using Vision Transformers (ViT) for global semantic understanding. By fusing these two frameworks in a Sequential Fusion Architecture, SafeVisionNet can capture minute visual details (e.g., blood textures, nudity, weapons) as well as overarching contextual semantics (e.g., violent intent, group aggression).

The hybridisation is purportedly to the advantage of the system in terms of accuracy, explainability, and generalizability across different social media scenarios, thus effectively circumventing the drawbacks of going with either CNN or transformer architectures. The entire flow (Fig. 1) is broken down into the following six stages:

1. data acquisition and annotation,

2. preprocessing and augmentation,

3. hybrid CNN–ViT architecture,

4. training and evaluation,

5. explainability and human-in-the-loop (HITL) integration, and

6. real-time deployment.

3.2 Data Acquisition and Annotation

In order to provide for the diversity and representativeness of the datasets, the datasets are procured from various publicly available repositories, and media platforms are scraped for data ethically. These sources comprise NSFW, Pornography-2K, Violent Scenes Dataset, and the APIs of Twitter, Reddit, and Flickr. All data collection processes are compliant with digital ethics and data protection laws.

Every image is assigned a label referring to one of the four classes in the taxonomy:

Safe — content that is benign and non-explicit.

Offensive — characterised by hate symbols, obscene gestures, or explicitly depicted imagery.

Violent — includes blood, weapons, or scenes of aggression.

Explicit — is made up of nudity or pornography.

For reliability, every single image is being annotated three times by three different sets of reviewers. Inter-annotator agreement is evaluated by means of Cohen’s κ, and the resolution of disagreements is done through majority consensus. Duplication is prevented by means of Perceptual Hashing (pHash). The data is split into 70% for learning, 15% for validation, and another 15% for testing through stratified sampling to maintain the balance of the classes.

3.3 Preprocessing and Data Augmentation

The quality of images on social media is not always consistent, and they may be affected by compression artefacts. Orientation noise may be present as well. For this reason, the preprocessing and augmentation steps are implemented:

Normalisation: Transform RGB pixel intensities to [0, 1] to attain numerical stability.

Resizing: Bring all data to 224 × 224 px so as to be compatible with EfficientNet-B0.

Data Augmentation: Random rotation (±15°), horizontal flipping, brightness jitter, and Gaussian blur are used to imitate real-life changes.

Noise Reduction: Median and bilateral filtering remove compression noise, with the helps of the edges are not blurred.

Class Rebalancing: The Synthetic Minority Oversampling Technique (SMOTE) takes care of the classes that are under-represented and thus ensures that learning is balanced.

The said transformations are instrumental in enhancing the generalizability of the model and in diminishing the chances of overfitting with repetitive visual patterns.

3.4​‍​‌‍​‍‌​‍​‌‍​‍‌ Hybrid CNN–ViT Architecture

SafeVisionNet’s design relies on a Sequential Feature Fusion methodology:

Stage 1: CNN Feature Extraction

By employing compound-scaled convolutions, the EfficientNet-B0 core can gather hierarchical features. The network’s lower layers are ones that basically find edges and textures of things, while the higher-level figures like blood stains, a gun, or hate signs are the things that the deeper layers ​‍​‌‍​‍‌​‍​‌‍​‍‌represent.

In a formal way, CNN feature maps

𝐹𝑐∈𝑅𝐻×𝑊×𝐷Fc∈RH×W×D

Undergo a 1 × 1 convolution projection:

𝐹𝑝=𝑊𝑝∗𝐹𝑐Fp​=Wp​∗Fc

where

𝑊𝑝Wp

​
It is a projection kernel that aligns the output dimension

𝐷𝑝Dp


With the embedding size of the transformer (e.g., 768).

Stage 2: Vision Transformer Encoding

The projected tensor is transformed into patch embeddings, with each patch considered a token. ViT encoder uses multi-head self-attention (MHSA) to understand the global context in a very long chain, thus it is able to infer, for example, the presence of aggression or threat to the whole scene.

At each time step:

𝑍𝑡=MHSA(𝐹𝑝)+FFN(𝑍𝑡−1)Zt=MHSA(Fp​)+FFN(Zt−1)

Where FFN stands for the feed-forward network. To keep the spatial order, positional encodings are added.

Stage 3: Classification Head

The last transformer output is fed into:

Layer Normalization

Dropout (p = 0.2)

Fully Connected (FC) Softmax Layer

That gives the class probabilities for Safe, Offensive, and Violent categories:

𝑃(𝑦∣𝑥)=Softmax(𝑊𝑐𝑍𝑡+𝑏𝑐)

P(y∣x)=Softmax(WcZt+bc)

So, the design fashion combines the spatial strength of CNN with the contextual power of ViT.

3.5 Model Training and Evaluation

The model uses Cross-Entropy Loss for training with the AdamW optimiser (learning rate = 1 × 10⁻⁴, weight decay = 0.01). Early stopping is used to stop the training process when the validation loss has been steady for 10 epochs. A cosine annealing scheduler is used for gradual learning.

Regularisation Techniques:

Dropout layers that are put in to prevent label smoothing (ε = 0.1) for calibration

Mixup augmentation that generates interpolated samples

Performance Metrics:

Accuracy (Acc) = (TP + TN)/(TP + FP + TN + FN)

Precision (P) = TP/(TP + FP)

Recall (R) = TP/(TP + FN)

F1-Score = 2PR/(P + R)

ROC-AUC for threshold optimisation

Evaluation Tools:

Confusion Matrix: class-wise diagnostic.

Grad-CAM and SHAP: show the parts of an image that influenced ​‍​‌‍​‍‌​‍​‌‍​‍‌a3.6 Explainability and Human-in-the-Loop (HITL) Integration

Explainability ensures moderator trust and ethical accountability. Grad-CAM overlays and attention visualisations identify which pixels or regions led to a classification. If model confidence < 0.65, the prediction is automatically routed to a HITL module, where human moderators validate or override the decision.

Feedback from these corrections is logged and fed into periodic incremental retraining cycles, enabling continual learning and bias reduction. This design harmonises AI efficiency with human judgment, aligning with AI Governance and EU AI Act transparency principles.

3.7 Real-Time Deployment and System Integration

The final model is containerised via TensorFlow Serving and deployed on AWS Cloud with GPU instances for scalable, real-time inference. Incoming image streams are managed by a Kafka message queue to ensure asynchronous, high-throughput processing.

Operational Pipeline:

Image ingestion via API gateway

Preprocessing & feature extraction on-the-fly

CNN-ViT inference and classification

Confidence scoring and heatmap generation

Human review (if needed)

Logging and retraining feedback

A web-based moderation dashboard displays:

Confidence probabilities

Heatmap visualizations

Moderation recommendations (Flag, Review, Approve)

System performance analytics

The RESTful interface allows integration with existing content moderation APIs and third-party tools like Google Perspective or Facebook’s Hateful Memes Toolkit.

3.8 Summary

The SafeVisionNet framework thus establishes a balanced, interpretable, and deployable system for social media content moderation. Its hybrid CNN–ViT fusion ensures both fine-grained sensitivity and global semantic understanding, while the HITL layer guarantees ethical oversight. Deployed on cloud infrastructure, the model supports real-time detection with explainable outputs—striking an equilibrium between user safety, fairness, and freedom of expression.



Fig. 1. Proposed Hybrid CNN–ViT Architecture (SafeVisionNet) for Offensive and Violent Image Detection.
The figure illustrates the Sequential Fusion Pipeline: input preprocessing → CNN-based local feature extraction → transformer encoding for global attention → Softmax classification → Grad-CAM visualisation → Human-in-the-Loop dashboard for ethical review.